# -*- coding: utf-8 -*-
"""EDA_Iowa_Liquor_Sales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/hamzafarooq/Time-Series/blob/master/EDA_Iowa_Liquor_Sales.ipynb

#Data Set Overivew

The tutorial uses the Iowa Liquor Retails Sales. We will be using it dataset to predict future sales for one of the stores

This dataset contains every wholesale purchase of liquor in the State of Iowa by retailers for sale to individuals since January 1, 2012.
The State of Iowa controls the wholesale distribution of liquor intended for retail sale, which means this dataset offers a complete view of retail liquor sales in the entire state. The dataset contains every wholesale order of liquor by all grocery stores, liquor stores, convenience stores, etc., with details about the store and location, the exact liquor brand and size, and the number of bottles ordered.

Run this every time you restart your notebook
"""

#!apt-get install libgeos-3.5.0
#!apt-get install libgeos-dev
#!pip install https://github.com/matplotlib/basemap/archive/master.zip
#!pip install pyproj==1.9.6

#!pip install gmplot

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
import itertools
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")
plt.style.use('fivethirtyeight')
import statsmodels.api as sm
import matplotlib
import datetime
import seaborn as sns

"""#Read Data
There are three ways to read data:

1. Use GCP directly and biq query
2. Google Drive Mount
3. CSV Path

## GCP
"""

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

# Commented out IPython magic to ensure Python compatibility.
# %%bigquery --project timeseriesmodeling-1 liquor_sales_orignial
# SELECT *
# FROM `bigquery-public-data.iowa_liquor_sales.sales`
# where  date > '2018-01-01'
#

"""## Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

liquor_sales_orignial=pd.read_csv('/content/drive/My Drive/Classroom/Time Series Modeling/liquor_sales.csv')

"""## Path to CSV"""

import pandas as pd

url = 'https://drive.google.com/file/d/1MRLQ4si-wDXGwUuZDUYUXXz9qNMIaT8z/view?usp=sharing'
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
liquor_sales_orignial = pd.read_csv(path)

"""# Data Overview"""

liquor_sales=liquor_sales_orignial # it is always useful to store an orignial version of the data

df_rows = liquor_sales.shape[0]

# Check for % of missing values in each column - drop nulls
liquor_sales.isnull().sum()/df_rows*100

liquor_sales = liquor_sales.dropna()
liquor_sales=liquor_sales.drop('Unnamed: 0',axis=1)

## check for data types and outliers for continuous variables
liquor_sales.dtypes

liquor_sales['Date'] = pd.to_datetime(liquor_sales['date'])
liquor_sales.drop('date',axis=1)

import seaborn as sns
# Use seaborn style defaults and set the default figure size
sns.set(rc={'figure.figsize':(11, 4)})

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt

liquor_sales.describe()

daily_sale=liquor_sales.groupby('Date')[['sale_dollars']].sum().sort_values(by = ['sale_dollars'], \
                                                                                ascending=False).reset_index()

daily_sale=daily_sale.sort_values('Date')
daily_sale.head(5)

plt.figure(figsize=(20,10))
plt.plot(daily_sale['Date'],daily_sale['sale_dollars'])
plt.show()

liquor_sales.head()

liquor_sales.columns

"""# Finding sales for all Wal-Mart Stores"""

wmt_list=[]
for i in set(liquor_sales['store_name']):
  if 'Wal-Mart' in i:
    wmt_list.append(i)

len(wmt_list)

wmt_stores_only = liquor_sales[liquor_sales['store_name'].isin(wmt_list)]
wmt_stores_only.head(5)

locations=set(wmt_stores_only['store_location'])

def long(q):
  l=[]
  for i in q:

    i=i.replace('POINT',"")
    i=i.replace('[',"")
    i=i.replace('(',"")
    k=i.split()
    l.append(float(k[0]))
  return l

def lat(q):
  l=[]
  for i in q:

    i=i.replace('POINT',"")
    i=i.replace('[',"")
    i=i.replace(')',"")
    k=i.split()
    l.append(float(k[1]))
  return l

wmt_stores_only['lat']= lat(wmt_stores_only['store_location'].values)
wmt_stores_only['long']= long(wmt_stores_only['store_location'].values)

wmt_locations=wmt_stores_only[['store_number','lat','long']].drop_duplicates()

wmt_store_sales = wmt_stores_only.groupby('store_number')[['sale_dollars']].sum().sort_values(by = ['sale_dollars'], \
                                                                                ascending=False).reset_index()

wmt_store_sales=pd.merge(wmt_store_sales,wmt_locations , how='left', on='store_number')

wmt_store_sales.head(5)

"""# Showing Geo-Location
The code for this section was taken from :https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.13-Geographic-Data-With-Basemap.ipynb#scrollTo=7R8Bx_Zmb0cM
"""

# Extract the data we're interested in
lat = wmt_store_sales['lat'].values
lon = wmt_store_sales['long'].values
sales = wmt_store_sales['sale_dollars'].values
#area = cities['area_total_km2'].values

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap

plt.figure(figsize=(8, 8))
m = Basemap(projection='ortho', resolution=None, lat_0=50, lon_0=-100)
m.bluemarble(scale=0.5);

# 1. Draw the map background
fig = plt.figure(figsize=(15, 15))
m = Basemap(projection='lcc', resolution='f', #40.6331° N, 89.3985° W
            lat_0=42, lon_0=-92,
            width=8E5, height=7E5)
m.shadedrelief()
m.drawcoastlines(color='gray')
m.drawcountries(color='gray')
m.drawstates(color='black')
# and size reflecting area
m.scatter(lon, lat, latlon=True,
          c=sales/5000,s=sales/5000,cmap='Reds', alpha=0.5)

# 3. create colorbar and legend
plt.colorbar(label=r'Sales x5000')
plt.clim(min(sales/5000), max(sales/5000))

# make legend with dummy points
for a in [100,200, 300, 400]:
    plt.scatter([], [], c='k', alpha=0.5, s=a,
                label=str(a) + ' x $10000')
plt.legend(scatterpoints=1, frameon=False,
           labelspacing=1, loc='lower left');

"""# Exploratory Data Analysis
### Top 5 categories of liquor by total sales

Credit: Batool Fatima
https://www.linkedin.com/in/batoolfatima1/
"""

liquor_sales=wmt_stores_only

liquor_sales['category_name'] = liquor_sales['category_name'].str.lower().str.title()

categories = pd.DataFrame(liquor_sales[['category_name','sale_dollars']])
Top_categories = categories.groupby('category_name')[['sale_dollars']].sum().sort_values(by = ['sale_dollars'], \
                                                                                ascending=False).reset_index()

Top_categories.shape

Top_categories.head()

categories_sales = Top_categories.head(10)
categories_sales

"""# Exploratory Data Analysis - Part 2


### Top Selling 10 categories
"""

# bar chart for total sales for each category
plt.figure(figsize=(10,10))
sns.barplot(x= 'category_name', y = 'sale_dollars', data=categories_sales)

# Rotate x-labels
plt.xticks(rotation=-45)

"""### Daily Sales"""

liquor_sales['year'] = pd.DatetimeIndex(liquor_sales['Date']).year
liquor_sales['month'] = pd.DatetimeIndex(liquor_sales['Date']).month
liquor_sales['day'] = pd.DatetimeIndex(liquor_sales['Date']).day_name()

daily_sales = liquor_sales.groupby('day')[['sale_dollars']].sum().sort_values(by = ['day']).reset_index()
#daily_sales

daysofweek = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# bar chart for total sales for each category
plt.figure(figsize=(8,6))
sns.barplot(x= 'day', y = 'sale_dollars', data=daily_sales, order = daysofweek)

# Rotate x-labels
plt.xticks(rotation=-45)

"""### Yearly trend of liquor categories"""

yearly_data = pd.DataFrame(liquor_sales[liquor_sales['category_name'].isin(['Canadian Whiskies', 'American Vodkas', \
                                                                            'Spiced Rum','Straight Bourbon Whiskies', \
                                                                            'Tennessee Whiskies'])])

x = yearly_data.groupby(['year', 'category_name'])['sale_dollars'].sum().reset_index()

plt.figure(figsize=(8,6))
sns.lineplot(x= 'year', y = 'sale_dollars', data=x, hue='category_name')

# Rotate x-labels
plt.xticks(rotation=-45)

"""## Drill deeper into the top selling categories - Canadian Whiskey"""

Can_whiskies_items = liquor_sales[liquor_sales['category_name'] == 'Canadian Whiskies']

Can_whiskies_items['item_description'] = Can_whiskies_items['item_description'].str.lower().str.title()

Can_whiskies_items.groupby(['item_description'])[['bottles_sold', 'sale_dollars']]\
.sum().sort_values(by = ['bottles_sold', 'sale_dollars'], ascending = False).reset_index().head(5)

Top_Whisky_items = pd.DataFrame(Can_whiskies_items[Can_whiskies_items['item_description'].isin\
                                      (['Black Velvet', 'Crown Royal', 'Black Velvet Toasted Caramel',\
                                      'Canadian Ltd Whisky','Crown Royal Regal Apple'])])

Top_Whisky_items1 = Top_Whisky_items.groupby(['item_description', 'year'])[['bottles_sold', 'sale_dollars']]\
.sum().sort_values(by = ['item_description', 'year', 'bottles_sold']).reset_index()
#Top_Whisky_items

len(Top_Whisky_items)

"""## bottles_sold for Top 5 Canadian Whisky Items"""

g = sns.FacetGrid(Top_Whisky_items, col="item_description", col_wrap = 3, height=4)
g.map(sns.pointplot, "year", "bottles_sold", alpha=.1)
g.add_legend();

"""## Sales for Top 5 Canadian Whisky Items"""

b = sns.FacetGrid(Top_Whisky_items, col="item_description", col_wrap = 3, height=4)
b.map(sns.pointplot, "year", "sale_dollars", alpha=.1)
b.add_legend();

"""## Average Markup for Canadian Whisky Items

"""

Top_Whisky_items['Average_Markup'] = ((Top_Whisky_items['state_bottle_retail'] - Top_Whisky_items['state_bottle_cost'])/Top_Whisky_items['bottles_sold'])/(Top_Whisky_items['state_bottle_cost']/Top_Whisky_items['bottles_sold'])

Top_Whisky_items.head()

Top_Whisky_Markup = Top_Whisky_items.groupby(['item_description'])[['Average_Markup']]\
.sum().sort_values(by = ['Average_Markup'], ascending = False).reset_index()

Top_Whisky_Markup