# -*- coding: utf-8 -*-
"""Op. entre Data Frames e Armazenamento

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GkzJYJ05mpfQ0YLQGujPuPMrAPYvPtLM

#1 instalação do PySpark
"""

!pip install pyspark

!pip install findspark
from pyspark import SparkContext
from pyspark.sql import SparkSession

sc = SparkContext.getOrCreate()

spark = SparkSession.builder.appName("PySpark Dataframe").getOrCreate()

"""#2 Transformation

#2.1 map()
"""

data = [1,2,3,4,5]
myRDD = sc.parallelize(data)
newRDD = myRDD.map(lambda x: x*2)
print(newRDD.collect())

"""#2.2 filter()"""

data = [1,2,3,4,5,6,7,8,9]
myRDD = sc.parallelize(data)
newRDD = myRDD.filter(lambda x: x%2 == 0)
print(newRDD.collect())

"""#2.3 distinct()"""

data = [1,1,1,2,2,2,2,3,3,3,3,4,5]
myRDD = sc.parallelize(data)
newRDD = myRDD.distinct()
print(newRDD.collect())

data = [1,1,1,2,2,2,2,3,3,3,3,4,5]
myRDD = sc.parallelize(data)
newRDD = myRDD.distinct()
print(newRDD.count())

data = [1,1,1,2,2,2,2,3,3,3,3,4,5]
myRDD = sc.parallelize(data)
newRDD = myRDD.distinct()
print(type(newRDD))

"""#2.4 groupByKey()"""

myRDD = sc.parallelize([("a", 1), ("a", 2), ("a", 3), ("b", 1)])

resultList = myRDD.groupByKey().mapValues(list)
resultList.collect()

"""#2.5 reduceByKey()"""

from operator import add
myRDD = sc.parallelize([("a", 1), ("a", 2), ("a", 3), ("b", 1)])

newRDD = myRDD.reduceByKey(add)
newRDD.collect()

"""#2.6 sortByKey()"""

from operator import add
myRDD = sc.parallelize([("a", 1), ("d", 2), ("c", 3), ("b", 1)])

newRDD = myRDD.sortByKey()
newRDD.collect()

"""#2.7 union()"""

myRDD1 = sc.parallelize([1,2,3,4])
myRDD2 = sc.parallelize([3,4,5,6,7])

newRDD = myRDD1.union(myRDD2)
newRDD.collect()

"""#3 Actions

#3.1 count()
"""

data = ["scala", "Python", "Java", "R"]
myRDD = sc.parallelize(data)
myRDD.count()

data = [1,2,3,3,4,5,5,6,7]
myRDD = sc.parallelize(data)
myRDD.count()

"""#3.2 reduce()"""

data = [1,2,3,4,5]
myRDD = sc.parallelize(data)
myRDD.reduce(lambda x, y : x * y)

"""#3.3 foreach()"""

def fun(x):
  print(x)
data = ["scala", "Python", "Java", "R"]
myRDD = sc.parallelize(data)
myRDD.foreach(fun)

"""#3.4 countByValue()"""

data = ["scala", "Python", "scala", "R", "Python", "Java", "Python", "R", "scala", "Python", "R"]
myRDD = sc.parallelize(data)
myRDD.countByValue().items()

"""#3.5 countbyKey()"""

data = [("a", 1), ("d", 1), ("c", 1), ("b", 1), ("b", 1)]
myRDD = sc.parallelize(data)
myRDD.countByKey().items()

"""#3.6 take(n)"""

data = [1,2,3,3,4,5,5,6,7]
myRDD = sc.parallelize(data)
myRDD.take(3)

"""#3.7 top(n)"""

data = [1,2,3,3,4,5,5,6,7]
myRDD = sc.parallelize(data)
myRDD.top(3)

"""## Na Prátrica como é o Apache *Spark*"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('Tutorial').getOrCreate()

dept = [("Finance",10),("Marketing",20),("Sales",30),("IT",40)]
rdd = spark.sparkContext.parallelize(dept)

df = rdd.toDF()
df.printSchema()
df.show(truncate=False)

deptColumns = ["dept_name","dept_id"]
df2 = rdd.toDF(deptColumns)
df2.printSchema()
df2.show(truncate=False)

deptDF = spark.createDataFrame(rdd, schema = deptColumns)
deptDF.printSchema()
deptDF.show(truncate=False)

from pyspark.sql.types import StructType,StructField, StringType
deptSchema = StructType([
    StructField('dept_name', StringType(), True),
    StructField('dept_id', StringType(), True)
])

deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)

from pyspark.sql.types import StructType,StructField, StringType
deptSchema = StructType([
    StructField('dept_name', StringType(), True),
    StructField('dept_id', StringType(), True)
])

deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)
deptDF1.printSchema()
deptDF1.show(truncate=False)