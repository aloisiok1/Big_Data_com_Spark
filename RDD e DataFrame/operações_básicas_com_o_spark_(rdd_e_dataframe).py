# -*- coding: utf-8 -*-
"""Operações Básicas com o Spark (RDD e DataFrame)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pMbnTC9_vdMtXcfd24t_5B6-8s6ggKc_

##1 Configurtação de Bibliotecas
"""

!pip install pyspark

from pyspark import SparkContext
from pyspark.sql import SparkSession

sc=SparkContext.getOrCreate()

spark=SparkSession.builder.appName("PySpark DataFrame From Rdd").getOrCreate()

"""##2 Create PySpark DataFrame from ans Existing RDD"""

rdd = sc.parallelize([("C", 85, 76, 87, 91), ("B", 85, 76, 87, 91), ("A", 85, 78, 96, 92), ("A", 92, 76, 89, 96)], 4)

print(type(rdd))

sub = ["id_person", "value_1", "value_2", "value_3", "value_4"]

marks_df = spark.createDataFrame(rdd, schema=sub)

print(type(marks_df))

marks_df.printSchema()

marks_df.show()

"""##3 Creation e Manipulation in PySpark DataFrame"""

!pip install pyspark
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("pysparkdf").getOrCreate()

"""##4 Importing Data"""

df = spark.read.csv("/content/cereal.csv", sep = ",", inferSchema = True, header = True)

"""##5 Reading de Schema"""

df.printSchema()

